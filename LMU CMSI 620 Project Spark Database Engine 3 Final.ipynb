{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57bdeaca",
   "metadata": {},
   "source": [
    "# Group 3 Project LMU CMSI Database Systems 620 \n",
    "# Fall 2023, Professor Latif, November 20, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b4a88",
   "metadata": {},
   "source": [
    "- Presentation and Project Database Engines \n",
    "    * Postgres\n",
    "    * MySQL\n",
    "    * Spark\n",
    "\n",
    "\n",
    "\n",
    "1. Dataset 1 from Brittany and Tanya\n",
    "    * Climate Change: Earth Surface Temperature Data Climate Change: Earth Surface Temperature Data (Kaggle)\n",
    "    * This dataset is 89 MB with global temperatures since 1750\n",
    "    \n",
    "\n",
    "1. Dataset 2 from Mariam\n",
    "    * CO₂ Emissions from Our World in Data Organization \n",
    "    * This dataset is 1.3MB with 9 columns, 30k rows and emissions since 1750\n",
    "\n",
    "1. Dataset 3 from Nidhi and Krutik\n",
    "    * Sea Level Change Change in Mean Sea Level from IMF.org (csv)\n",
    "    * The dataset is 14.6 MB with 13 columns and around 35K rows.\n",
    "    \n",
    "    \n",
    "Notes: \n",
    "1. Spark doesn't have constraints like primary or foreign keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa8db1",
   "metadata": {},
   "source": [
    "# Spark Database Engine #3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d4bb8",
   "metadata": {},
   "source": [
    "Apache Spark is a unified engine designed for large-scale distributed data processing, on premises in data centers or in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5f1649",
   "metadata": {},
   "source": [
    "Initial Steps Taken:\n",
    "1. Download Apache Spark using https://spark.apache.org/downloads.html\n",
    "2. %pip  install pyspark\n",
    "3. %pip install --upgrade pip and homebrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "12f4ac52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a7772a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import datetime\n",
    "\n",
    "from os.path import abspath\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import to_date, col, when, col, regexp_replace, year, expr, year, month, dayofmonth, concat, lpad, to_date, date_format\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ccb3a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/26 12:04:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "warehouse_location = abspath('spark-warehouse')\n",
    "dataset_size_in_bytes = 1073741824\n",
    "\n",
    "def calculate_partitions(dataset_size_in_bytes, bytes_per_partition=10000000):\n",
    "    return max(1, int(dataset_size_in_bytes / bytes_per_partition))\n",
    "\n",
    "spark = (SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CMSI 620: Database Engine\") \\\n",
    "    .config(\"spark.executor.memory\", \"11g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"4g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.1\") \\\n",
    "    .config(\"spark.executor.instances\", \"5\") \\\n",
    "    .config(\"spark.executor.cores\", \"5\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"100m\") \\\n",
    "    .config(\"spark.sql.inMemoryColumnarStorage.compressed\", \"true\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", calculate_partitions(dataset_size_in_bytes)) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dabd210",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10 * 1024 * 1024  # 10 MB\n",
    "if dataset_size_in_bytes < threshold:\n",
    "    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", dataset_size_in_bytes)\n",
    "else:\n",
    "    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a09758e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext.setLogLevel(\"INFO\")\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "# spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099c5d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mariams-mbp-2.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CMSI 620: Database Engine</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=CMSI 620: Database Engine>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675e4238",
   "metadata": {},
   "source": [
    "## Methods and Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d78a9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE = \"cmsi620_gpr3_db\"  \n",
    "\n",
    "GLOBAL_TEMP_CSV_FILE_LOCATION = 'GlobalLandTemperaturesByCity.csv'\n",
    "EMISSIONS_CSV_FILE_LOCATION = 'CO2.csv'\n",
    "SEA_LEVEL_CSV_FILE_LOCATION = 'sea_level_new.csv'\n",
    "SEA_COUNTRY_CSV_FILE_LOCATION = 'country_sea.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d80be136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_update_cols(df, schema_cols):\n",
    "    for field in schema_cols.fields:\n",
    "        print(field.name, field.name)\n",
    "        df = df.withColumnRenamed(field.name, field.name)\n",
    "    return df\n",
    "\n",
    "def get_tables(db):\n",
    "    tables = spark.catalog.listTables(db)\n",
    "    print(tables)\n",
    "    for table in tables:\n",
    "        print(table.name)\n",
    "    return None\n",
    "\n",
    "def get_databases():\n",
    "    databases = spark.catalog.listDatabases()\n",
    "    for db in databases:\n",
    "        print(db.name)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2e8b7",
   "metadata": {},
   "source": [
    "## Create A Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd7e2249",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('create database CMSI620_GPR3_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d4bfb50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[info_name: string, info_value: string]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('describe database extended CMSI620_GPR3_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da3406",
   "metadata": {},
   "source": [
    "## Check Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "187d05fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='cmsi620_gpr3_db', catalog='spark_catalog', description='', locationUri='file:/Users/mariamjoan/Desktop/Spark/spark-warehouse/cmsi620_gpr3_db.db'),\n",
       " Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='file:/Users/mariamjoan/Desktop/Spark/spark-warehouse')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c796b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|      namespace|\n",
      "+---------------+\n",
      "|cmsi620_gpr3_db|\n",
      "|        default|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5278b578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cmsi620_gpr3_db'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d88a2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/26 11:57:15 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/26 11:57:15 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/11/26 11:57:17 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/11/26 11:57:17 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore mariamjoan@192.168.1.53\n",
      "23/11/26 11:57:17 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use cmsi620_gpr3_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42cb94be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cmsi620_gpr3_db'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5449a51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f55b27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='src', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36525d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default|      src|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables from default').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd85eb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------------------+-----------+\n",
      "|namespace      |tableName                         |isTemporary|\n",
      "+---------------+----------------------------------+-----------+\n",
      "|cmsi620_gpr3_db|global_co2_emissions_country_f    |false      |\n",
      "|cmsi620_gpr3_db|global_co2_emissions_country_sum_f|false      |\n",
      "|cmsi620_gpr3_db|global_temp_city_f                |false      |\n",
      "|cmsi620_gpr3_db|global_temp_f                     |false      |\n",
      "|cmsi620_gpr3_db|sea_level_country_f               |false      |\n",
      "|cmsi620_gpr3_db|sea_level_f                       |false      |\n",
      "+---------------+----------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables from cmsi620_gpr3_db').show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d7b402",
   "metadata": {},
   "source": [
    "# Dataset #1: Global Temperature\n",
    "### Climate Change: Earth Surface Temperature Data Climate Change: Earth Surface Temperature Data (Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77e328df",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_city_schema = StructType([\n",
    "    StructField(\"dt\", DateType(), True),\n",
    "    StructField(\"AverageTemperature\", FloatType(), True),\n",
    "    StructField(\"AverageTemperatureUncertainty\", FloatType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"Latitude\", StringType(), True),\n",
    "    StructField(\"Longitude\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f152fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    path=GLOBAL_TEMP_CSV_FILE_LOCATION,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    encoding=\"UTF-8\",\n",
    "    schema =global_city_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4ddf88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = df.selectExpr(\n",
    "    \"City as city\",\n",
    "    \"Country as country\",\n",
    "    \"Latitude as latitude\",\n",
    "    \"Longitude as longitude\"\n",
    ").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ea7910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = city.dropDuplicates([\"city\", \"country\", \"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41c9fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.selectExpr(\n",
    "    \"City as city\",\n",
    "    \"Country as country\",\n",
    "    \"dt as date_measured\",\n",
    "    \"AverageTemperature as avg_temp\",\n",
    "    \"AverageTemperatureUncertainty as avg_temp_uncertainty\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d9e42d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.filter(col(\"avg_temp\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3d7bd44",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/25 20:54:32 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/11/25 20:54:32 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(4, City#35, Country#36, Latitude#37, Longitude#38)\n",
      "23/11/25 20:54:32 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "23/11/25 20:54:32 INFO CodeGenerator: Code generated in 21.36901 ms\n",
      "23/11/25 20:54:32 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 200.8 KiB, free 1223.1 MiB)\n",
      "23/11/25 20:54:32 INFO BlockManagerInfo: Removed broadcast_11_piece0 on mariams-mbp-2.lan:56985 in memory (size: 5.8 KiB, free: 1223.6 MiB)\n",
      "23/11/25 20:54:32 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1223.1 MiB)\n",
      "23/11/25 20:54:32 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on mariams-mbp-2.lan:56985 (size: 34.6 KiB, free: 1223.5 MiB)\n",
      "23/11/25 20:54:32 INFO SparkContext: Created broadcast 12 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/11/25 20:54:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 44752064 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/11/25 20:54:32 INFO BlockManagerInfo: Removed broadcast_10_piece0 on mariams-mbp-2.lan:56985 in memory (size: 9.0 KiB, free: 1223.5 MiB)\n",
      "23/11/25 20:54:32 INFO DAGScheduler: Registering RDD 21 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "23/11/25 20:54:32 INFO DAGScheduler: Got map stage job 10 (showString at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "23/11/25 20:54:32 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/11/25 20:54:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/25 20:54:32 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/25 20:54:32 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[21] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/11/25 20:54:32 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 35.7 KiB, free 1223.1 MiB)\n",
      "23/11/25 20:54:32 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 1223.1 MiB)\n",
      "23/11/25 20:54:32 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on mariams-mbp-2.lan:56985 (size: 16.4 KiB, free: 1223.5 MiB)\n",
      "23/11/25 20:54:32 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
      "23/11/25 20:54:32 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[21] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "23/11/25 20:54:32 INFO TaskSchedulerImpl: Adding task set 11.0 with 12 tasks resource profile 0\n",
      "23/11/25 20:54:32 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 26) (mariams-mbp-2.lan, executor driver, partition 0, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:32 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 27) (mariams-mbp-2.lan, executor driver, partition 1, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:32 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 28) (mariams-mbp-2.lan, executor driver, partition 2, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:32 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 29) (mariams-mbp-2.lan, executor driver, partition 3, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:32 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 30) (mariams-mbp-2.lan, executor driver, partition 4, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:32 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 31) (mariams-mbp-2.lan, executor driver, partition 5, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:32 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 32) (mariams-mbp-2.lan, executor driver, partition 6, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:32 INFO TaskSetManager: Starting task 7.0 in stage 11.0 (TID 33) (mariams-mbp-2.lan, executor driver, partition 7, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:32 INFO TaskSetManager: Starting task 8.0 in stage 11.0 (TID 34) (mariams-mbp-2.lan, executor driver, partition 8, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:32 INFO TaskSetManager: Starting task 9.0 in stage 11.0 (TID 35) (mariams-mbp-2.lan, executor driver, partition 9, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:32 INFO TaskSetManager: Starting task 10.0 in stage 11.0 (TID 36) (mariams-mbp-2.lan, executor driver, partition 10, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:32 INFO TaskSetManager: Starting task 11.0 in stage 11.0 (TID 37) (mariams-mbp-2.lan, executor driver, partition 11, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:32 INFO Executor: Running task 0.0 in stage 11.0 (TID 26)\n",
      "23/11/25 20:54:32 INFO Executor: Running task 1.0 in stage 11.0 (TID 27)\n",
      "23/11/25 20:54:32 INFO Executor: Running task 2.0 in stage 11.0 (TID 28)\n",
      "23/11/25 20:54:32 INFO Executor: Running task 3.0 in stage 11.0 (TID 29)\n",
      "23/11/25 20:54:32 INFO Executor: Running task 5.0 in stage 11.0 (TID 31)\n",
      "23/11/25 20:54:32 INFO Executor: Running task 4.0 in stage 11.0 (TID 30)\n",
      "23/11/25 20:54:32 INFO Executor: Running task 6.0 in stage 11.0 (TID 32)\n",
      "23/11/25 20:54:32 INFO Executor: Running task 7.0 in stage 11.0 (TID 33)\n",
      "23/11/25 20:54:32 INFO Executor: Running task 8.0 in stage 11.0 (TID 34)\n",
      "23/11/25 20:54:32 INFO Executor: Running task 9.0 in stage 11.0 (TID 35)\n",
      "23/11/25 20:54:32 INFO Executor: Running task 10.0 in stage 11.0 (TID 36)\n",
      "23/11/25 20:54:32 INFO Executor: Running task 11.0 in stage 11.0 (TID 37)\n",
      "23/11/25 20:54:32 INFO CodeGenerator: Code generated in 10.445236 ms\n",
      "23/11/25 20:54:32 INFO CodeGenerator: Code generated in 4.606467 ms\n",
      "23/11/25 20:54:32 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 223760320-268512384, partition values: [empty row]\n",
      "23/11/25 20:54:32 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 0-44752064, partition values: [empty row]\n",
      "23/11/25 20:54:32 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 358016512-402768576, partition values: [empty row]\n",
      "23/11/25 20:54:32 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 179008256-223760320, partition values: [empty row]\n",
      "23/11/25 20:54:32 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 492272704-532830464, partition values: [empty row]\n",
      "23/11/25 20:54:32 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 402768576-447520640, partition values: [empty row]\n",
      "23/11/25 20:54:32 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 134256192-179008256, partition values: [empty row]\n",
      "23/11/25 20:54:32 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 447520640-492272704, partition values: [empty row]\n",
      "23/11/25 20:54:32 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 89504128-134256192, partition values: [empty row]\n",
      "23/11/25 20:54:32 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 44752064-89504128, partition values: [empty row]\n",
      "23/11/25 20:54:32 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 268512384-313264448, partition values: [empty row]\n",
      "23/11/25 20:54:32 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 313264448-358016512, partition values: [empty row]\n",
      "23/11/25 20:54:33 INFO BlockManagerInfo: Removed broadcast_9_piece0 on mariams-mbp-2.lan:56985 in memory (size: 34.6 KiB, free: 1223.6 MiB)\n",
      "23/11/25 20:54:36 INFO Executor: Finished task 11.0 in stage 11.0 (TID 37). 2773 bytes result sent to driver\n",
      "23/11/25 20:54:36 INFO TaskSetManager: Finished task 11.0 in stage 11.0 (TID 37) in 3584 ms on mariams-mbp-2.lan (executor driver) (1/12)\n",
      "23/11/25 20:54:36 INFO Executor: Finished task 9.0 in stage 11.0 (TID 35). 2773 bytes result sent to driver\n",
      "23/11/25 20:54:36 INFO TaskSetManager: Finished task 9.0 in stage 11.0 (TID 35) in 3932 ms on mariams-mbp-2.lan (executor driver) (2/12)\n",
      "23/11/25 20:54:36 INFO Executor: Finished task 7.0 in stage 11.0 (TID 33). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:36 INFO TaskSetManager: Finished task 7.0 in stage 11.0 (TID 33) in 3965 ms on mariams-mbp-2.lan (executor driver) (3/12)\n",
      "23/11/25 20:54:36 INFO Executor: Finished task 1.0 in stage 11.0 (TID 27). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:36 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 27) in 3971 ms on mariams-mbp-2.lan (executor driver) (4/12)\n",
      "23/11/25 20:54:36 INFO Executor: Finished task 10.0 in stage 11.0 (TID 36). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:36 INFO TaskSetManager: Finished task 10.0 in stage 11.0 (TID 36) in 3980 ms on mariams-mbp-2.lan (executor driver) (5/12)\n",
      "23/11/25 20:54:36 INFO Executor: Finished task 8.0 in stage 11.0 (TID 34). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:36 INFO TaskSetManager: Finished task 8.0 in stage 11.0 (TID 34) in 4007 ms on mariams-mbp-2.lan (executor driver) (6/12)\n",
      "23/11/25 20:54:36 INFO Executor: Finished task 2.0 in stage 11.0 (TID 28). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:36 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 28) in 4010 ms on mariams-mbp-2.lan (executor driver) (7/12)\n",
      "23/11/25 20:54:36 INFO Executor: Finished task 4.0 in stage 11.0 (TID 30). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:36 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 30) in 4026 ms on mariams-mbp-2.lan (executor driver) (8/12)\n",
      "23/11/25 20:54:36 INFO Executor: Finished task 6.0 in stage 11.0 (TID 32). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:36 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 32) in 4028 ms on mariams-mbp-2.lan (executor driver) (9/12)\n",
      "23/11/25 20:54:36 INFO Executor: Finished task 3.0 in stage 11.0 (TID 29). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:36 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 29) in 4033 ms on mariams-mbp-2.lan (executor driver) (10/12)\n",
      "23/11/25 20:54:36 INFO Executor: Finished task 5.0 in stage 11.0 (TID 31). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:36 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 31) in 4040 ms on mariams-mbp-2.lan (executor driver) (11/12)\n",
      "23/11/25 20:54:37 INFO Executor: Finished task 0.0 in stage 11.0 (TID 26). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:37 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 26) in 4087 ms on mariams-mbp-2.lan (executor driver) (12/12)\n",
      "23/11/25 20:54:37 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "23/11/25 20:54:37 INFO DAGScheduler: ShuffleMapStage 11 (showString at NativeMethodAccessorImpl.java:0) finished in 4.095 s\n",
      "23/11/25 20:54:37 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/11/25 20:54:37 INFO DAGScheduler: running: Set()\n",
      "23/11/25 20:54:37 INFO DAGScheduler: waiting: Set()\n",
      "23/11/25 20:54:37 INFO DAGScheduler: failed: Set()\n",
      "23/11/25 20:54:37 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/11/25 20:54:37 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "23/11/25 20:54:37 INFO CodeGenerator: Code generated in 17.939861 ms\n",
      "23/11/25 20:54:37 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/11/25 20:54:37 INFO DAGScheduler: Got job 11 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/11/25 20:54:37 INFO DAGScheduler: Final stage: ResultStage 13 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/11/25 20:54:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "23/11/25 20:54:37 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/25 20:54:37 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[24] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/11/25 20:54:37 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 42.1 KiB, free 1223.3 MiB)\n",
      "23/11/25 20:54:37 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 18.9 KiB, free 1223.3 MiB)\n",
      "23/11/25 20:54:37 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on mariams-mbp-2.lan:56985 (size: 18.9 KiB, free: 1223.5 MiB)\n",
      "23/11/25 20:54:37 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535\n",
      "23/11/25 20:54:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[24] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/11/25 20:54:37 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "23/11/25 20:54:37 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 38) (mariams-mbp-2.lan, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "23/11/25 20:54:37 INFO Executor: Running task 0.0 in stage 13.0 (TID 38)\n",
      "23/11/25 20:54:37 INFO ShuffleBlockFetcherIterator: Getting 12 (272.4 KiB) non-empty blocks including 12 (272.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/11/25 20:54:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "23/11/25 20:54:37 INFO Executor: Finished task 0.0 in stage 13.0 (TID 38). 6122 bytes result sent to driver\n",
      "23/11/25 20:54:37 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 38) in 43 ms on mariams-mbp-2.lan (executor driver) (1/1)\n",
      "23/11/25 20:54:37 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "23/11/25 20:54:37 INFO DAGScheduler: ResultStage 13 (showString at NativeMethodAccessorImpl.java:0) finished in 0.051 s\n",
      "23/11/25 20:54:37 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/25 20:54:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "23/11/25 20:54:37 INFO DAGScheduler: Job 11 finished: showString at NativeMethodAccessorImpl.java:0, took 0.061163 s\n",
      "23/11/25 20:54:37 INFO FileSourceStrategy: Pushed Filters:                      \n",
      "23/11/25 20:54:37 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(4, City#35, Country#36, Latitude#37, Longitude#38)\n",
      "23/11/25 20:54:37 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "23/11/25 20:54:37 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 200.8 KiB, free 1223.1 MiB)\n",
      "23/11/25 20:54:37 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1223.0 MiB)\n",
      "23/11/25 20:54:37 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on mariams-mbp-2.lan:56985 (size: 34.6 KiB, free: 1223.5 MiB)\n",
      "23/11/25 20:54:37 INFO SparkContext: Created broadcast 15 from count at NativeMethodAccessorImpl.java:0\n",
      "23/11/25 20:54:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 44752064 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/11/25 20:54:37 INFO DAGScheduler: Registering RDD 28 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "23/11/25 20:54:37 INFO DAGScheduler: Got map stage job 12 (count at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "23/11/25 20:54:37 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (count at NativeMethodAccessorImpl.java:0)\n",
      "23/11/25 20:54:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/25 20:54:37 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/25 20:54:37 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[28] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/11/25 20:54:37 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 35.7 KiB, free 1223.0 MiB)\n",
      "23/11/25 20:54:37 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 1223.0 MiB)\n",
      "23/11/25 20:54:37 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on mariams-mbp-2.lan:56985 (size: 16.4 KiB, free: 1223.5 MiB)\n",
      "23/11/25 20:54:37 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535\n",
      "23/11/25 20:54:37 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[28] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "23/11/25 20:54:37 INFO TaskSchedulerImpl: Adding task set 14.0 with 12 tasks resource profile 0\n",
      "23/11/25 20:54:37 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 39) (mariams-mbp-2.lan, executor driver, partition 0, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:37 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 40) (mariams-mbp-2.lan, executor driver, partition 1, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:37 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 41) (mariams-mbp-2.lan, executor driver, partition 2, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:37 INFO TaskSetManager: Starting task 3.0 in stage 14.0 (TID 42) (mariams-mbp-2.lan, executor driver, partition 3, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:37 INFO TaskSetManager: Starting task 4.0 in stage 14.0 (TID 43) (mariams-mbp-2.lan, executor driver, partition 4, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:37 INFO TaskSetManager: Starting task 5.0 in stage 14.0 (TID 44) (mariams-mbp-2.lan, executor driver, partition 5, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:37 INFO TaskSetManager: Starting task 6.0 in stage 14.0 (TID 45) (mariams-mbp-2.lan, executor driver, partition 6, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:37 INFO TaskSetManager: Starting task 7.0 in stage 14.0 (TID 46) (mariams-mbp-2.lan, executor driver, partition 7, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:37 INFO TaskSetManager: Starting task 8.0 in stage 14.0 (TID 47) (mariams-mbp-2.lan, executor driver, partition 8, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:37 INFO TaskSetManager: Starting task 9.0 in stage 14.0 (TID 48) (mariams-mbp-2.lan, executor driver, partition 9, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:37 INFO TaskSetManager: Starting task 10.0 in stage 14.0 (TID 49) (mariams-mbp-2.lan, executor driver, partition 10, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:37 INFO TaskSetManager: Starting task 11.0 in stage 14.0 (TID 50) (mariams-mbp-2.lan, executor driver, partition 11, PROCESS_LOCAL, 7940 bytes) \n",
      "23/11/25 20:54:37 INFO Executor: Running task 0.0 in stage 14.0 (TID 39)\n",
      "23/11/25 20:54:37 INFO Executor: Running task 4.0 in stage 14.0 (TID 43)\n",
      "23/11/25 20:54:37 INFO Executor: Running task 6.0 in stage 14.0 (TID 45)\n",
      "23/11/25 20:54:37 INFO Executor: Running task 7.0 in stage 14.0 (TID 46)\n",
      "23/11/25 20:54:37 INFO Executor: Running task 8.0 in stage 14.0 (TID 47)\n",
      "23/11/25 20:54:37 INFO Executor: Running task 9.0 in stage 14.0 (TID 48)\n",
      "23/11/25 20:54:37 INFO Executor: Running task 1.0 in stage 14.0 (TID 40)\n",
      "23/11/25 20:54:37 INFO Executor: Running task 10.0 in stage 14.0 (TID 49)\n",
      "23/11/25 20:54:37 INFO Executor: Running task 2.0 in stage 14.0 (TID 41)\n",
      "23/11/25 20:54:37 INFO Executor: Running task 5.0 in stage 14.0 (TID 44)\n",
      "23/11/25 20:54:37 INFO Executor: Running task 11.0 in stage 14.0 (TID 50)\n",
      "23/11/25 20:54:37 INFO Executor: Running task 3.0 in stage 14.0 (TID 42)\n",
      "23/11/25 20:54:37 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 44752064-89504128, partition values: [empty row]\n",
      "23/11/25 20:54:37 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 402768576-447520640, partition values: [empty row]\n",
      "23/11/25 20:54:37 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 268512384-313264448, partition values: [empty row]\n",
      "23/11/25 20:54:37 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 447520640-492272704, partition values: [empty row]\n",
      "23/11/25 20:54:37 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 0-44752064, partition values: [empty row]\n",
      "23/11/25 20:54:37 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 89504128-134256192, partition values: [empty row]\n",
      "23/11/25 20:54:37 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 134256192-179008256, partition values: [empty row]\n",
      "23/11/25 20:54:37 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 358016512-402768576, partition values: [empty row]\n",
      "23/11/25 20:54:37 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 313264448-358016512, partition values: [empty row]\n",
      "23/11/25 20:54:37 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 223760320-268512384, partition values: [empty row]\n",
      "23/11/25 20:54:37 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 179008256-223760320, partition values: [empty row]\n",
      "23/11/25 20:54:37 INFO FileScanRDD: Reading File path: file:///Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv, range: 492272704-532830464, partition values: [empty row]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------+---------+\n",
      "|           city|       country|latitude|longitude|\n",
      "+---------------+--------------+--------+---------+\n",
      "|       Ayacucho|          Peru|  13.66S|   73.49W|\n",
      "|         Bantou|         China|  24.92N|  118.82E|\n",
      "|       Ajdabiya|         Libya|  31.35N|   20.73E|\n",
      "|         Ambala|         India|  29.74N|   77.54E|\n",
      "|       Akishima|         Japan|  36.17N|  139.23E|\n",
      "|      Amagasaki|         Japan|  34.56N|  136.22E|\n",
      "|         Arnhem|   Netherlands|  52.24N|    5.26E|\n",
      "|          Baneh|          Iran|  36.17N|   45.75E|\n",
      "|      Barcelona|     Venezuela|  10.45N|   64.64W|\n",
      "|       Acarigua|     Venezuela|   8.84N|   68.92W|\n",
      "|     Almetyevsk|        Russia|  55.45N|   51.02E|\n",
      "|         Awassa|      Ethiopia|   7.23N|   38.11E|\n",
      "|         Abadan|          Iran|  29.74N|   48.00E|\n",
      "|       Aberdeen|United Kingdom|  57.05N|    1.48W|\n",
      "|       Adilabad|         India|  20.09N|   78.48E|\n",
      "|          Aktau|    Kazakhstan|  44.20N|   51.43E|\n",
      "|        Anaheim| United States|  32.95N|  117.77W|\n",
      "|        Antakya|        Turkey|  36.17N|   35.80E|\n",
      "|         Astana|    Kazakhstan|  50.63N|   72.25E|\n",
      "|Barrancabermeja|      Colombia|   7.23N|   73.78W|\n",
      "+---------------+--------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/25 20:54:37 INFO BlockManagerInfo: Removed broadcast_14_piece0 on mariams-mbp-2.lan:56985 in memory (size: 18.9 KiB, free: 1223.5 MiB)\n",
      "23/11/25 20:54:40 INFO Executor: Finished task 11.0 in stage 14.0 (TID 50). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Finished task 11.0 in stage 14.0 (TID 50) in 3110 ms on mariams-mbp-2.lan (executor driver) (1/12)\n",
      "23/11/25 20:54:40 INFO Executor: Finished task 9.0 in stage 14.0 (TID 48). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Finished task 9.0 in stage 14.0 (TID 48) in 3368 ms on mariams-mbp-2.lan (executor driver) (2/12)\n",
      "23/11/25 20:54:40 INFO Executor: Finished task 0.0 in stage 14.0 (TID 39). 2773 bytes result sent to driver\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 39) in 3469 ms on mariams-mbp-2.lan (executor driver) (3/12)\n",
      "23/11/25 20:54:40 INFO Executor: Finished task 6.0 in stage 14.0 (TID 45). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Finished task 6.0 in stage 14.0 (TID 45) in 3484 ms on mariams-mbp-2.lan (executor driver) (4/12)\n",
      "23/11/25 20:54:40 INFO Executor: Finished task 10.0 in stage 14.0 (TID 49). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Finished task 10.0 in stage 14.0 (TID 49) in 3515 ms on mariams-mbp-2.lan (executor driver) (5/12)\n",
      "23/11/25 20:54:40 INFO Executor: Finished task 3.0 in stage 14.0 (TID 42). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Finished task 3.0 in stage 14.0 (TID 42) in 3528 ms on mariams-mbp-2.lan (executor driver) (6/12)\n",
      "23/11/25 20:54:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on mariams-mbp-2.lan:56985 in memory (size: 34.6 KiB, free: 1223.5 MiB)\n",
      "23/11/25 20:54:40 INFO BlockManagerInfo: Removed broadcast_13_piece0 on mariams-mbp-2.lan:56985 in memory (size: 16.4 KiB, free: 1223.6 MiB)\n",
      "23/11/25 20:54:40 INFO Executor: Finished task 7.0 in stage 14.0 (TID 46). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Finished task 7.0 in stage 14.0 (TID 46) in 3562 ms on mariams-mbp-2.lan (executor driver) (7/12)\n",
      "23/11/25 20:54:40 INFO Executor: Finished task 2.0 in stage 14.0 (TID 41). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Finished task 2.0 in stage 14.0 (TID 41) in 3564 ms on mariams-mbp-2.lan (executor driver) (8/12)\n",
      "23/11/25 20:54:40 INFO Executor: Finished task 1.0 in stage 14.0 (TID 40). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 40) in 3591 ms on mariams-mbp-2.lan (executor driver) (9/12)\n",
      "23/11/25 20:54:40 INFO Executor: Finished task 5.0 in stage 14.0 (TID 44). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Finished task 5.0 in stage 14.0 (TID 44) in 3601 ms on mariams-mbp-2.lan (executor driver) (10/12)\n",
      "23/11/25 20:54:40 INFO Executor: Finished task 8.0 in stage 14.0 (TID 47). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Finished task 8.0 in stage 14.0 (TID 47) in 3602 ms on mariams-mbp-2.lan (executor driver) (11/12)\n",
      "23/11/25 20:54:40 INFO Executor: Finished task 4.0 in stage 14.0 (TID 43). 2730 bytes result sent to driver\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Finished task 4.0 in stage 14.0 (TID 43) in 3606 ms on mariams-mbp-2.lan (executor driver) (12/12)\n",
      "23/11/25 20:54:40 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "23/11/25 20:54:40 INFO DAGScheduler: ShuffleMapStage 14 (count at NativeMethodAccessorImpl.java:0) finished in 3.613 s\n",
      "23/11/25 20:54:40 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/11/25 20:54:40 INFO DAGScheduler: running: Set()\n",
      "23/11/25 20:54:40 INFO DAGScheduler: waiting: Set()\n",
      "23/11/25 20:54:40 INFO DAGScheduler: failed: Set()\n",
      "23/11/25 20:54:40 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/11/25 20:54:40 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "23/11/25 20:54:40 INFO CodeGenerator: Code generated in 14.296332 ms\n",
      "23/11/25 20:54:40 INFO DAGScheduler: Registering RDD 31 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "23/11/25 20:54:40 INFO DAGScheduler: Got map stage job 13 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/11/25 20:54:40 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (count at NativeMethodAccessorImpl.java:0)\n",
      "23/11/25 20:54:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\n",
      "23/11/25 20:54:40 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/25 20:54:40 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[31] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/11/25 20:54:40 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 45.3 KiB, free 1223.3 MiB)\n",
      "23/11/25 20:54:40 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 20.6 KiB, free 1223.3 MiB)\n",
      "23/11/25 20:54:40 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on mariams-mbp-2.lan:56985 (size: 20.6 KiB, free: 1223.5 MiB)\n",
      "23/11/25 20:54:40 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535\n",
      "23/11/25 20:54:40 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[31] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/11/25 20:54:40 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 51) (mariams-mbp-2.lan, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
      "23/11/25 20:54:40 INFO Executor: Running task 0.0 in stage 16.0 (TID 51)\n",
      "23/11/25 20:54:40 INFO ShuffleBlockFetcherIterator: Getting 12 (272.4 KiB) non-empty blocks including 12 (272.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/11/25 20:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "23/11/25 20:54:40 INFO Executor: Finished task 0.0 in stage 16.0 (TID 51). 5656 bytes result sent to driver\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 51) in 35 ms on mariams-mbp-2.lan (executor driver) (1/1)\n",
      "23/11/25 20:54:40 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "23/11/25 20:54:40 INFO DAGScheduler: ShuffleMapStage 16 (count at NativeMethodAccessorImpl.java:0) finished in 0.042 s\n",
      "23/11/25 20:54:40 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/11/25 20:54:40 INFO DAGScheduler: running: Set()\n",
      "23/11/25 20:54:40 INFO DAGScheduler: waiting: Set()\n",
      "23/11/25 20:54:40 INFO DAGScheduler: failed: Set()\n",
      "23/11/25 20:54:40 INFO CodeGenerator: Code generated in 8.265942 ms\n",
      "23/11/25 20:54:40 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "23/11/25 20:54:40 INFO DAGScheduler: Got job 14 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/11/25 20:54:40 INFO DAGScheduler: Final stage: ResultStage 19 (count at NativeMethodAccessorImpl.java:0)\n",
      "23/11/25 20:54:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\n",
      "23/11/25 20:54:40 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/25 20:54:40 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[34] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/11/25 20:54:40 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 12.1 KiB, free 1223.2 MiB)\n",
      "23/11/25 20:54:40 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 1223.2 MiB)\n",
      "23/11/25 20:54:40 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on mariams-mbp-2.lan:56985 (size: 5.8 KiB, free: 1223.5 MiB)\n",
      "23/11/25 20:54:40 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1535\n",
      "23/11/25 20:54:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[34] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/11/25 20:54:40 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 52) (mariams-mbp-2.lan, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "23/11/25 20:54:40 INFO Executor: Running task 0.0 in stage 19.0 (TID 52)\n",
      "23/11/25 20:54:40 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/11/25 20:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "23/11/25 20:54:40 INFO Executor: Finished task 0.0 in stage 19.0 (TID 52). 3995 bytes result sent to driver\n",
      "23/11/25 20:54:40 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 52) in 7 ms on mariams-mbp-2.lan (executor driver) (1/1)\n",
      "23/11/25 20:54:40 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "23/11/25 20:54:40 INFO DAGScheduler: ResultStage 19 (count at NativeMethodAccessorImpl.java:0) finished in 0.011 s\n",
      "23/11/25 20:54:40 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/25 20:54:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "23/11/25 20:54:40 INFO DAGScheduler: Job 14 finished: count at NativeMethodAccessorImpl.java:0, took 0.014883 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3510"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city.show()\n",
    "city.printSchema()\n",
    "city.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b07274e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------------+--------+--------------------+\n",
      "| city|country|date_measured|avg_temp|avg_temp_uncertainty|\n",
      "+-----+-------+-------------+--------+--------------------+\n",
      "|Århus|Denmark|   1743-11-01|   6.068|               1.737|\n",
      "|Århus|Denmark|   1744-04-01|   5.788|               3.624|\n",
      "|Århus|Denmark|   1744-05-01|  10.644|               1.283|\n",
      "|Århus|Denmark|   1744-06-01|  14.051|               1.347|\n",
      "|Århus|Denmark|   1744-07-01|  16.082|               1.396|\n",
      "|Århus|Denmark|   1744-09-01|  12.781|               1.454|\n",
      "|Århus|Denmark|   1744-10-01|    7.95|                1.63|\n",
      "|Århus|Denmark|   1744-11-01|   4.639|               1.302|\n",
      "|Århus|Denmark|   1744-12-01|   0.122|               1.756|\n",
      "|Århus|Denmark|   1745-01-01|  -1.333|               1.642|\n",
      "|Århus|Denmark|   1745-02-01|  -2.732|               1.358|\n",
      "|Århus|Denmark|   1745-03-01|   0.129|               1.088|\n",
      "|Århus|Denmark|   1745-04-01|   4.042|               1.138|\n",
      "|Århus|Denmark|   1750-01-01|   1.699|               1.013|\n",
      "|Århus|Denmark|   1750-02-01|   3.961|               2.361|\n",
      "|Århus|Denmark|   1750-03-01|   5.182|                3.48|\n",
      "|Århus|Denmark|   1750-04-01|   7.197|               0.732|\n",
      "|Århus|Denmark|   1750-05-01|  10.634|               1.351|\n",
      "|Århus|Denmark|   1750-06-01|  14.913|               1.181|\n",
      "|Århus|Denmark|   1750-07-01|  17.831|                1.22|\n",
      "+-----+-------+-------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- date_measured: date (nullable = true)\n",
      " |-- avg_temp: float (nullable = true)\n",
      " |-- avg_temp_uncertainty: float (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8235082"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.show()\n",
    "temp.printSchema()\n",
    "temp.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ba7c7",
   "metadata": {},
   "source": [
    "## Dataset #1 Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cef94a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = \"date_measured\"\n",
    "temp = temp.withColumn(date_col, regexp_replace(col(date_col), \"-\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4123f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(city='Århus', country='Denmark', date_measured='17431101', avg_temp=6.067999839782715, avg_temp_uncertainty=1.7369999885559082)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef1de304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/26 10:50:34 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 9:====================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+-------------------+------------------+--------------------+\n",
      "|summary|    city|    country|      date_measured|          avg_temp|avg_temp_uncertainty|\n",
      "+-------+--------+-----------+-------------------+------------------+--------------------+\n",
      "|  count| 8235082|    8235082|            8235082|           8235082|             8235082|\n",
      "|   mean|    null|       null|1.910898935810014E7| 16.72743263617215|  1.0285747414276099|\n",
      "| stddev|    null|       null|  641959.9052579077|10.353442482004715|  1.1297332887072418|\n",
      "|    min|A Coruña|Afghanistan|           17431101|           -42.704|               0.034|\n",
      "|    max|  Ürümqi|   Zimbabwe|           20130901|            39.651|              15.396|\n",
      "+-------+--------+-----------+-------------------+------------------+--------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [City#3 AS city#84, Country#4 AS country#85, regexp_replace(cast(dt#0 as string), -, , 1) AS date_measured#131, AverageTemperature#1 AS avg_temp#87, AverageTemperatureUncertainty#2 AS avg_temp_uncertainty#88]\n",
      "+- *(1) Filter isnotnull(AverageTemperature#1)\n",
      "   +- FileScan csv [dt#0,AverageTemperature#1,AverageTemperatureUncertainty#2,City#3,Country#4] Batched: false, DataFilters: [isnotnull(AverageTemperature#1)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/mariamjoan/Desktop/Spark/GlobalLandTemperaturesByCity.csv], PartitionFilters: [], PushedFilters: [IsNotNull(AverageTemperature)], ReadSchema: struct<dt:date,AverageTemperature:float,AverageTemperatureUncertainty:float,City:string,Country:s...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp.describe().show()\n",
    "temp.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81657a8d",
   "metadata": {},
   "source": [
    "## Save Dataset #1 as Table in Spark Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fba6eb43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "city.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE}.global_temp_city_f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc2278bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/26 10:51:40 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/11/26 10:51:40 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/11/26 10:51:40 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/26 10:51:40 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "temp.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE}.global_temp_f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071faba",
   "metadata": {},
   "source": [
    "# Dataset #2 Co2 Emissions \n",
    "## Global CO2 Emissions Data By Year and Country Across Multiple Domains, e.g Oil, Gas, Coal, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c15900ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_schema = StructType([\n",
    "    StructField(\"Entity\", StringType(), True),\n",
    "    StructField(\"Code\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"Annual_CO2_other_industry_other\", FloatType(), True),\n",
    "    StructField(\"Annual_CO2_other_industry_flaring\", FloatType(), True),\n",
    "    StructField(\"Annual_CO2_other_industry_cement\", FloatType(), True),\n",
    "    StructField(\"Annual_CO2_other_industry_gas\", FloatType(), True),\n",
    "    StructField(\"Annual_CO2_other_industry_oil\", FloatType(), True),\n",
    "    StructField(\"Annual_CO2_other_industry_coal\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37e22522",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2 = spark.read.csv(\n",
    "    path=EMISSIONS_CSV_FILE_LOCATION,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    encoding=\"UTF-8\",\n",
    "    schema =emissions_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84fe4285",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2tx = co2.selectExpr(\n",
    "    \"Entity as country\",\n",
    "    \"Year as year_measured\",\n",
    "    \"Annual_CO2_other_industry_other as co2_other_yr\",\n",
    "    \"Annual_CO2_other_industry_flaring as co2_flaring_yr\",\n",
    "    \"Annual_CO2_other_industry_cement as co2_cement_yr\",\n",
    "    \"Annual_CO2_other_industry_gas as co2_gas_yr\",\n",
    "    \"Annual_CO2_other_industry_oil as co2_oil_yr\",\n",
    "    \"Annual_CO2_other_industry_coal as co2_coal_yr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fb584f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+--------------+-------------+----------+----------+-----------+\n",
      "|    country|year_measured|co2_other_yr|co2_flaring_yr|co2_cement_yr|co2_gas_yr|co2_oil_yr|co2_coal_yr|\n",
      "+-----------+-------------+------------+--------------+-------------+----------+----------+-----------+\n",
      "|Afghanistan|         1949|        null|           0.0|          0.0|       0.0|       0.0|    14656.0|\n",
      "|Afghanistan|         1950|        null|           0.0|          0.0|       0.0| 63203.996|    21068.0|\n",
      "|Afghanistan|         1951|        null|           0.0|          0.0|       0.0|   65952.0|    25648.0|\n",
      "|Afghanistan|         1952|        null|           0.0|          0.0|       0.0|   59892.0|  31707.998|\n",
      "|Afghanistan|         1953|        null|           0.0|          0.0|       0.0|   68307.0|    37949.0|\n",
      "|Afghanistan|         1954|        null|           0.0|          0.0|       0.0|   63754.0|    42502.0|\n",
      "|Afghanistan|         1955|        null|           0.0|          0.0|       0.0|   91600.0|    62288.0|\n",
      "|Afghanistan|         1956|        null|           0.0|          0.0|       0.0|  120912.0|    62288.0|\n",
      "|Afghanistan|         1957|        null|           0.0|          0.0|       0.0|  216176.0|    76944.0|\n",
      "|Afghanistan|         1958|        null|           0.0|          0.0|       0.0|  238160.0|    91600.0|\n",
      "|Afghanistan|         1959|        null|           0.0|      18171.0|       0.0| 256480.02|   109920.0|\n",
      "|Afghanistan|         1960|        null|           0.0|      18012.0|       0.0|  268758.0|   127115.0|\n",
      "|Afghanistan|         1961|        null|           0.0|      21806.0|       0.0|  293120.0|   175872.0|\n",
      "|Afghanistan|         1962|        null|           0.0|      29074.0|       0.0|  362736.0|   296784.0|\n",
      "|Afghanistan|         1963|        null|           0.0|      50880.0|       0.0|  392048.0|   263808.0|\n",
      "|Afghanistan|         1964|        null|           0.0|      61783.0|       0.0|  476320.0|   300448.0|\n",
      "|Afghanistan|         1965|        null|           0.0|      83589.0|       0.0|  542272.0|   381056.0|\n",
      "|Afghanistan|         1966|        null|           0.0|      87223.0|       0.0|  575248.0|   428688.0|\n",
      "|Afghanistan|         1967|        null|           0.0|      65417.0|  260144.0|  556928.0|   399376.0|\n",
      "|Afghanistan|         1968|        null|           0.0|      47105.0|  347041.0|  496817.0|   332429.0|\n",
      "+-----------+-------------+------------+--------------+-------------+----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- year_measured: integer (nullable = true)\n",
      " |-- co2_other_yr: float (nullable = true)\n",
      " |-- co2_flaring_yr: float (nullable = true)\n",
      " |-- co2_cement_yr: float (nullable = true)\n",
      " |-- co2_gas_yr: float (nullable = true)\n",
      " |-- co2_oil_yr: float (nullable = true)\n",
      " |-- co2_coal_yr: float (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/26 10:53:29 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Entity, Year, Annual_CO2_industry_other, Annual_CO2_flaring, Annual_CO2_other_cement, Annual_CO2_other_gas, Annual_CO2_other_oil, Annual_CO2_other_coal\n",
      " Schema: Entity, Year, Annual_CO2_other_industry_other, Annual_CO2_other_industry_flaring, Annual_CO2_other_industry_cement, Annual_CO2_other_industry_gas, Annual_CO2_other_industry_oil, Annual_CO2_other_industry_coal\n",
      "Expected: Annual_CO2_other_industry_other but found: Annual_CO2_industry_other\n",
      "CSV file: file:///Users/mariamjoan/Desktop/Spark/CO2.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29457"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co2tx.show()\n",
    "co2tx.printSchema()\n",
    "co2tx.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f7b45a",
   "metadata": {},
   "source": [
    "## Dataset #2 Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45b3a5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/26 10:54:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Entity, Year, Annual_CO2_industry_other, Annual_CO2_flaring, Annual_CO2_other_cement, Annual_CO2_other_gas, Annual_CO2_other_oil, Annual_CO2_other_coal\n",
      " Schema: Entity, Year, Annual_CO2_other_industry_other, Annual_CO2_other_industry_flaring, Annual_CO2_other_industry_cement, Annual_CO2_other_industry_gas, Annual_CO2_other_industry_oil, Annual_CO2_other_industry_coal\n",
      "Expected: Annual_CO2_other_industry_other but found: Annual_CO2_industry_other\n",
      "CSV file: file:///Users/mariamjoan/Desktop/Spark/CO2.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|summary|    country|     year_measured|        co2_other_yr|      co2_flaring_yr|      co2_cement_yr|          co2_gas_yr|          co2_oil_yr|         co2_coal_yr|\n",
      "+-------+-----------+------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|  count|      29457|             29457|                2386|               24875|              24974|               24943|               25042|               25069|\n",
      "|   mean|       null|1937.0758393590659|1.8430048331300262E7|   3493834.438078785|  8392489.821707202|   5.0080670938516E7|1.1138560219004871E8|1.5439769972069743E8|\n",
      "| stddev|       null| 67.96719006257644| 4.186320591625684E7|2.0297655092618626E7|6.300943499326838E7|3.0987496653119504E8| 6.250209486950399E8| 7.514527249915178E8|\n",
      "|    min|Afghanistan|              1750|                 0.0|                 0.0|                0.0|                 0.0|                 0.0|                 0.0|\n",
      "|    max|   Zimbabwe|              2021|        3.06638592E8|        4.39253984E8|       1.67259238E9|         7.9218294E9|       1.23456532E10|       1.50515128E10|\n",
      "+-------+-----------+------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [Entity#716 AS country#734, Year#718 AS year_measured#735, Annual_CO2_other_industry_other#719 AS co2_other_yr#736, Annual_CO2_other_industry_flaring#720 AS co2_flaring_yr#737, Annual_CO2_other_industry_cement#721 AS co2_cement_yr#738, Annual_CO2_other_industry_gas#722 AS co2_gas_yr#739, Annual_CO2_other_industry_oil#723 AS co2_oil_yr#740, Annual_CO2_other_industry_coal#724 AS co2_coal_yr#741]\n",
      "+- FileScan csv [Entity#716,Year#718,Annual_CO2_other_industry_other#719,Annual_CO2_other_industry_flaring#720,Annual_CO2_other_industry_cement#721,Annual_CO2_other_industry_gas#722,Annual_CO2_other_industry_oil#723,Annual_CO2_other_industry_coal#724] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/mariamjoan/Desktop/Spark/CO2.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Entity:string,Year:int,Annual_CO2_other_industry_other:float,Annual_CO2_other_industry_fla...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "co2tx.describe().show()\n",
    "co2tx.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1fb4e4",
   "metadata": {},
   "source": [
    "## Save Dataset #2 as Table in Spark Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e6bb14f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/26 10:54:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Entity, Year, Annual_CO2_industry_other, Annual_CO2_flaring, Annual_CO2_other_cement, Annual_CO2_other_gas, Annual_CO2_other_oil, Annual_CO2_other_coal\n",
      " Schema: Entity, Year, Annual_CO2_other_industry_other, Annual_CO2_other_industry_flaring, Annual_CO2_other_industry_cement, Annual_CO2_other_industry_gas, Annual_CO2_other_industry_oil, Annual_CO2_other_industry_coal\n",
      "Expected: Annual_CO2_other_industry_other but found: Annual_CO2_industry_other\n",
      "CSV file: file:///Users/mariamjoan/Desktop/Spark/CO2.csv\n"
     ]
    }
   ],
   "source": [
    "co2tx.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE}.global_co2_emissions_country_f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63dc35",
   "metadata": {},
   "source": [
    "### Build Emissions Table to Fit Schema and ERD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac63486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_dm = spark.sql(\"\"\" \n",
    "select\n",
    "     monotonically_increasing_id() as emissions_id \n",
    "    ,country\n",
    "    ,year_measured\n",
    "    ,co2_coal_yr\n",
    "    ,co2_oil_yr\n",
    "    ,co2_gas_yr\n",
    "    ,co2_cement_yr\n",
    "    ,co2_flaring_yr\n",
    "    ,co2_other_yr\n",
    "    ,sum(co2_coal_yr + co2_oil_yr + co2_gas_yr + co2_cement_yr + co2_flaring_yr + co2_other_yr) cumulative_co2\n",
    "from cmsi620_gpr3_db.global_co2_emissions_country_f\n",
    "group by 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d20c2a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_dm.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE}.global_co2_emissions_country_sum_f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59901260",
   "metadata": {},
   "source": [
    "# Dataset #3 Sea Level Changes by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1c47062",
   "metadata": {},
   "outputs": [],
   "source": [
    "sea_level_schema = StructType([\n",
    "    StructField(\"ObjectId\", IntegerType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "#     StructField(\"ISO2\", IntegerType(), True),\n",
    "#     StructField(\"ISO3\", FloatType(), True),\n",
    "#     StructField(\"Indicator\", FloatType(), True),\n",
    "#     StructField(\"Unit\", FloatType(), True),\n",
    "#     StructField(\"Source\", FloatType(), True),\n",
    "#     StructField(\"CTS_Code\", FloatType(), True),\n",
    "#     StructField(\"CTS_Name\", FloatType(), True),\n",
    "#     StructField(\"CTS_Full_Descriptor\", FloatType(), True),\n",
    "    StructField(\"Measure\", StringType(), True),\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Value\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a463f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sea = spark.read.csv(\n",
    "    path=SEA_LEVEL_CSV_FILE_LOCATION,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    encoding=\"UTF-8\",\n",
    "    schema =sea_level_schema\n",
    ")\n",
    "\n",
    "sea_country = spark.read.csv(\n",
    "    path=SEA_COUNTRY_CSV_FILE_LOCATION,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    encoding=\"UTF-8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba4ff9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seat = sea.selectExpr(\n",
    "    \"ObjectId as id\",\n",
    "    \"Country as country\",\n",
    "    \"Measure as sea_name\",\n",
    "    \"Date    as date_measured\",\n",
    "    \"Value as sea_level\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04040fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "seat = seat.withColumn(\n",
    "    \"date_measured\",\n",
    "    date_format(to_date(col(\"date_measured\"), 'MM/dd/yyyy'), 'yyyyMMdd')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "581fe518",
   "metadata": {},
   "outputs": [],
   "source": [
    "seat = seat.withColumn(\n",
    "    \"year_measured\",\n",
    "    year(to_date(\"date_measured\", 'yyyyMMdd'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12d20c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|country|          sea|\n",
      "+-------+-------------+\n",
      "| Belize|Caribbean Sea|\n",
      "+-------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- sea: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sea_country.show(1)\n",
    "sea_country.printSchema()\n",
    "sea_country.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02992513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------------+---------+-------------+\n",
      "| id|country|      sea_name|date_measured|sea_level|year_measured|\n",
      "+---+-------+--------------+-------------+---------+-------------+\n",
      "|  1|  World|   Andaman Sea|     19921217|   -10.34|         1992|\n",
      "|  2|  World|   Arabian Sea|     19921217|   -18.46|         1992|\n",
      "|  3|  World|Atlantic Ocean|     19921217|   -15.41|         1992|\n",
      "+---+-------+--------------+-------------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- sea_name: string (nullable = true)\n",
      " |-- date_measured: string (nullable = true)\n",
      " |-- sea_level: float (nullable = true)\n",
      " |-- year_measured: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35604"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seat.show(3)\n",
    "seat.printSchema()\n",
    "seat.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e7ae21",
   "metadata": {},
   "source": [
    "## Save Dataset #3 as Table in Spark Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "285f610d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seat.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE}.sea_level_f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b34febb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/25 23:13:36 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/11/25 23:13:36 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/11/25 23:13:36 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/25 23:13:36 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "sea_country.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE}.sea_level_country_f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7540f633",
   "metadata": {},
   "source": [
    "# Performance \n",
    "1. Using TimeIt from Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f310e",
   "metadata": {},
   "source": [
    "## Dataset #1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34a48a2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.03 ms ± 105 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "query = \"select * from cmsi620_gpr3_db.global_temp_f\"\n",
    "result = spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263beee8",
   "metadata": {},
   "source": [
    "## Dataset #2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "49fbc99f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.46 ms ± 206 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "query = f\"select * from {DATABASE}.global_co2_emissions_country_sum_f\"\n",
    "result = spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c4046",
   "metadata": {},
   "source": [
    "## Dataset # 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7db92511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.11 ms ± 334 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "query = f\"select * from {DATABASE}.sea_level_f\"\n",
    "result = spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafc9d94",
   "metadata": {},
   "source": [
    "## All Datasets # 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "32fce696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.3 ms ± 11.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "query = spark.sql(\"\"\"\n",
    "with base as ( \n",
    "select\n",
    "     coalesce( f.country, e.country, f.country )  country  \n",
    "    ,s.sea_name \n",
    "    ,cast( f.date_measured as int ) as date_measured\n",
    "    ,year( to_date( f.date_measured, 'yyyyMMdd')) as year_measured\n",
    "    \n",
    "    ,max( f.avg_temp  )  avg_temp_celsius\n",
    "    ,max( round( ((f.avg_temp * 9/5) + 32) ,2)) avg_temp_faranheit\n",
    "    ,max( cast( e.cumulative_co2 as int ))  cumulative_co2\n",
    "    ,max( s.sea_level )  sea_level_mm \n",
    "    ,max( (s.sea_level *1.0 / 25.4 ) ) as sea_level_inches\n",
    "from cmsi620_gpr3_db.global_temp_f f\n",
    "    left join cmsi620_gpr3_db.global_temp_city_f c on f.country = c.country    \n",
    "    left join cmsi620_gpr3_db.sea_level_country_f l on f.country  = l.country\n",
    "    left join cmsi620_gpr3_db.sea_level_f s         on s.sea_name = l.sea and f.date_measured = s.date_measured\n",
    "    left join cmsi620_gpr3_db.global_co2_emissions_country_sum_f e on f.country = e.country and year( to_date( f.date_measured, 'yyyyMMdd')) = e.year_measured\n",
    "group by 1, 2, 3, 4\n",
    ")\n",
    "\n",
    "select \n",
    "     country\n",
    "    ,sea_name\n",
    "    ,year_measured\n",
    "    ,date_measured\n",
    "    ,avg_temp_faranheit\n",
    "    ,avg_temp_celsius\n",
    "    ,cumulative_co2\n",
    "    ,sea_level_mm\n",
    "    ,sea_level_inches\n",
    "    ,row_number() over( partition by country order by year_measured, date_measured desc ) as rn \n",
    "from base\n",
    "order by 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "51b8904d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.9 ms ± 2.44 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "query = spark.sql(\"\"\"\n",
    "with base as ( \n",
    "select\n",
    "     coalesce( f.country, e.country, f.country )  country  \n",
    "    ,s.sea_name \n",
    "    ,cast( f.date_measured as int ) as date_measured\n",
    "    ,year( to_date( f.date_measured, 'yyyyMMdd')) as year_measured\n",
    "    \n",
    "    ,f.avg_temp    avg_temp_celsius\n",
    "    ,round( ((f.avg_temp * 9/5) + 32) ,2) avg_temp_faranheit\n",
    "    ,cast( e.cumulative_co2 as int )  cumulative_co2\n",
    "    ,s.sea_level   sea_level_mm \n",
    "    ,(s.sea_level *1.0 / 25.4 )  as sea_level_inches\n",
    "from cmsi620_gpr3_db.global_temp_f f\n",
    "    left join cmsi620_gpr3_db.global_temp_city_f c on f.country = c.country    \n",
    "    left join cmsi620_gpr3_db.sea_level_country_f l on f.country  = l.country\n",
    "    left join cmsi620_gpr3_db.sea_level_f s         on s.sea_name = l.sea and f.date_measured = s.date_measured\n",
    "    left join cmsi620_gpr3_db.global_co2_emissions_country_sum_f e on f.country = e.country and year( to_date( f.date_measured, 'yyyyMMdd')) = e.year_measured\n",
    ")\n",
    "\n",
    "select \n",
    "     country\n",
    "    ,sea_name\n",
    "    ,year_measured\n",
    "    ,date_measured\n",
    "    ,avg_temp_faranheit\n",
    "    ,avg_temp_celsius\n",
    "    ,cumulative_co2\n",
    "    ,sea_level_mm\n",
    "    ,sea_level_inches\n",
    "    ,row_number() over( partition by country order by year_measured, date_measured desc ) as rn \n",
    "from base\n",
    "order by 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0a9cc",
   "metadata": {},
   "source": [
    "# Querying All (3) Datasets\n",
    "- LEFT JOIN and GROUPBY and WINDOW FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "daf86022",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fq = spark.sql(\"\"\"\n",
    "with base as ( \n",
    "select\n",
    "     coalesce( f.country, e.country, f.country )  country  \n",
    "    ,s.sea_name \n",
    "    ,cast( f.date_measured as int ) as date_measured\n",
    "    ,year( to_date( f.date_measured, 'yyyyMMdd')) as year_measured\n",
    "    \n",
    "    ,max( f.avg_temp  )  avg_temp_celsius\n",
    "    ,max( cast( e.cumulative_co2 as int ))  cumulative_co2\n",
    "    ,max( s.sea_level )  sea_level_mm \n",
    "    ,max( (s.sea_level *1.0 / 25.4 ) ) as sea_level_inches\n",
    "from cmsi620_gpr3_db.global_temp_f f\n",
    "    left join cmsi620_gpr3_db.global_temp_city_f c on f.country = c.country    \n",
    "    left join cmsi620_gpr3_db.sea_level_country_f l on f.country  = l.country\n",
    "    left join cmsi620_gpr3_db.sea_level_f s         on s.sea_name = l.sea and f.date_measured = s.date_measured\n",
    "    left join cmsi620_gpr3_db.global_co2_emissions_country_sum_f e on f.country = e.country and year( to_date( f.date_measured, 'yyyyMMdd')) = e.year_measured\n",
    "where 1=1\n",
    "    and sea_name is not null\n",
    "group by 1, 2, 3, 4\n",
    ")\n",
    "\n",
    "select \n",
    "     country\n",
    "    ,sea_name\n",
    "    ,year_measured\n",
    "    ,date_measured\n",
    "    ,avg_temp_faranheit\n",
    "    ,avg_temp_celsius\n",
    "    ,cumulative_co2\n",
    "    ,sea_level_mm\n",
    "    ,sea_level_inches\n",
    "    ,row_number() over( partition by country order by year_measured, date_measured desc ) as rn \n",
    "from base\n",
    "order by 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8406d242",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 237:==================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-------------+-------------+------------------+----------------+--------------+------------+---------------------+---+\n",
      "|country  |sea_name      |year_measured|date_measured|avg_temp_faranheit|avg_temp_celsius|cumulative_co2|sea_level_mm|sea_level_inches     |rn |\n",
      "+---------+--------------+-------------+-------------+------------------+----------------+--------------+------------+---------------------+---+\n",
      "|Albania  |Adriatic Sea  |1993         |19931001     |65.44             |18.577          |null          |5.99        |0.23582676264244742  |1  |\n",
      "|Albania  |Adriatic Sea  |1993         |19930901     |69.92             |21.068          |null          |-11.91      |-0.46889763178787836 |2  |\n",
      "|Albania  |Adriatic Sea  |1993         |19930701     |75.13             |23.961          |null          |-89.61      |-3.527952779935101   |3  |\n",
      "|Albania  |Adriatic Sea  |1994         |19941201     |49.58             |9.764           |null          |24.19       |0.9523622257502999   |4  |\n",
      "|Albania  |Adriatic Sea  |1994         |19941101     |56.14             |13.412          |null          |33.19       |1.3066928593192515   |5  |\n",
      "|Albania  |Adriatic Sea  |1997         |19970601     |73.36             |22.978          |null          |-46.61      |-1.8350393941083294  |6  |\n",
      "|Albania  |Adriatic Sea  |1999         |19991001     |64.83             |18.24           |null          |68.99       |2.716141648179903    |7  |\n",
      "|Albania  |Adriatic Sea  |2000         |20001201     |51.83             |11.016          |null          |130.49      |5.137401791069451    |8  |\n",
      "|Albania  |Adriatic Sea  |2002         |20020501     |65.81             |18.783          |null          |-28.11      |-1.106692937415416   |9  |\n",
      "|Albania  |Adriatic Sea  |2002         |20020401     |57.12             |13.958          |null          |-93.51      |-3.6814961470956877  |10 |\n",
      "|Albania  |Adriatic Sea  |2002         |20020101     |44.7              |7.056           |null          |-8.01       |-0.31535433971975735 |11 |\n",
      "|Albania  |Adriatic Sea  |2003         |20030601     |77.66             |25.364          |null          |-8.61       |-0.33897636443611207 |12 |\n",
      "|Albania  |Adriatic Sea  |2003         |20030101     |49.75             |9.862           |null          |133.49      |5.255512027289924    |13 |\n",
      "|Albania  |Adriatic Sea  |2004         |20040401     |58.31             |14.615          |null          |-56.31      |-2.216929187924843   |14 |\n",
      "|Albania  |Adriatic Sea  |2005         |20050701     |76.97             |24.983          |null          |-3.81       |-0.14999999774722603 |15 |\n",
      "|Albania  |Adriatic Sea  |2005         |20050601     |71.54             |21.966          |null          |-64.21      |-2.5279527198611285  |16 |\n",
      "|Albania  |Adriatic Sea  |2007         |20070301     |54.51             |12.506          |null          |14.39       |0.56653544658751     |17 |\n",
      "|Albania  |Adriatic Sea  |2008         |20080201     |48.67             |9.263           |null          |-57.11      |-2.248425220879983   |18 |\n",
      "|Albania  |Adriatic Sea  |2009         |20091001     |61.76             |16.535          |null          |73.09       |2.877558910940576    |19 |\n",
      "|Albania  |Adriatic Sea  |2009         |20090901     |72.03             |22.24           |null          |62.29       |2.4523622407687933   |20 |\n",
      "|Albania  |Adriatic Sea  |2009         |20090601     |71.39             |21.885          |null          |-8.67       |-0.341338585680864   |21 |\n",
      "|Albania  |Adriatic Sea  |2010         |20101201     |50.26             |10.146          |null          |207.29      |8.161023357721765    |22 |\n",
      "|Albania  |Adriatic Sea  |2010         |20101101     |59.41             |15.225          |null          |110.59      |4.3539368636964815   |23 |\n",
      "|Albania  |Adriatic Sea  |2010         |20100801     |78.5              |25.832          |null          |9.43        |0.37125985453447957  |24 |\n",
      "|Albania  |Adriatic Sea  |2010         |20100701     |77.44             |25.246          |null          |32.33       |1.2728347177580586   |25 |\n",
      "|Albania  |Adriatic Sea  |2011         |20111001     |61.26             |16.255          |null          |26.63       |1.0484251638097088   |26 |\n",
      "|Albania  |Adriatic Sea  |2011         |20110901     |75.4              |24.11           |null          |94.53       |3.721653495247909    |27 |\n",
      "|Albania  |Adriatic Sea  |2012         |20121201     |47.58             |8.653           |null          |58.93       |2.3200787521722748   |28 |\n",
      "|Albania  |Adriatic Sea  |2012         |20121101     |59.51             |15.284          |null          |165.03      |6.497244046429011    |29 |\n",
      "|Albania  |Adriatic Sea  |2012         |20120801     |79.82             |26.566          |null          |55.49       |2.1846457353727087   |30 |\n",
      "|Albania  |Adriatic Sea  |2012         |20120601     |74.89             |23.829          |null          |10.69       |0.420866125211941    |31 |\n",
      "|Albania  |Adriatic Sea  |2012         |20120101     |44.04             |6.688           |null          |16.39       |0.645275566521592    |32 |\n",
      "|Angola   |Atlantic Ocean|1996         |19960101     |77.45             |25.251          |null          |-10.31      |-0.4059055283313661  |1  |\n",
      "|Angola   |Atlantic Ocean|1997         |19970501     |75.35             |24.085          |null          |-15.01      |-0.5909448909008597  |2  |\n",
      "|Angola   |Atlantic Ocean|1997         |19970401     |78.46             |25.809          |null          |-29.31      |-1.1539369868481253  |3  |\n",
      "|Angola   |Atlantic Ocean|1997         |19970201     |78.63             |25.904          |null          |-1.91       |-0.07519684907958264 |4  |\n",
      "|Angola   |Atlantic Ocean|1998         |19980701     |71.61             |22.007          |null          |-2.81       |-0.11062991900706855 |5  |\n",
      "|Angola   |Atlantic Ocean|1998         |19980601     |74.62             |23.68           |null          |-29.21      |-1.1499999639556164  |6  |\n",
      "|Angola   |Atlantic Ocean|2000         |20001001     |76.42             |24.677          |null          |11.79       |0.4641732268446074   |7  |\n",
      "|Angola   |Atlantic Ocean|2002         |20020301     |81.56             |27.533          |null          |18.99       |0.7476377862644947   |8  |\n",
      "|Angola   |Atlantic Ocean|2002         |20020101     |78.31             |25.729          |null          |23.79       |0.9366142092727301   |9  |\n",
      "|Angola   |Atlantic Ocean|2003         |20030501     |78.49             |25.827          |null          |25.09       |0.9877952815979485   |10 |\n",
      "|Angola   |Atlantic Ocean|2003         |20030401     |81.69             |27.607          |null          |-2.61       |-0.10275590138172541 |11 |\n",
      "|Angola   |Atlantic Ocean|2003         |20030201     |80.7              |27.057          |null          |-0.55       |-0.021653543776414527|12 |\n",
      "|Angola   |Atlantic Ocean|2004         |20040701     |68.54             |20.302          |null          |9.15        |0.3602362054539478   |13 |\n",
      "|Angola   |Atlantic Ocean|2004         |20040601     |70.8              |21.553          |null          |-2.15       |-0.08464567304596188 |14 |\n",
      "|Angola   |Atlantic Ocean|2005         |20050801     |70.33             |21.295          |null          |4.89        |0.19251967978289747  |15 |\n",
      "|Angola   |Atlantic Ocean|2009         |20090501     |77.78             |25.436          |null          |15.52       |0.6110236400694359   |16 |\n",
      "|Angola   |Atlantic Ocean|2009         |20090401     |80.02             |26.676          |null          |9.62        |0.37874015297476704  |17 |\n",
      "|Angola   |Atlantic Ocean|2009         |20090201     |77.65             |25.362          |null          |19.02       |0.7488189156599871   |18 |\n",
      "|Angola   |Atlantic Ocean|2010         |20100701     |70.68             |21.488          |null          |29.12       |1.1464567259540708   |19 |\n",
      "|Angola   |Atlantic Ocean|2010         |20100601     |73.37             |22.985          |null          |23.72       |0.9338582406832477   |20 |\n",
      "|Angola   |Atlantic Ocean|2011         |20110801     |70.22             |21.234          |null          |17.92       |0.7055118140273208   |21 |\n",
      "|Angola   |Atlantic Ocean|2011         |20110301     |80.81             |27.115          |null          |26.05       |1.025590521144116    |22 |\n",
      "|Angola   |Atlantic Ocean|2012         |20121101     |78.76             |25.976          |null          |69.75       |2.7460629921259843   |23 |\n",
      "|Angola   |Atlantic Ocean|2012         |20121001     |76.39             |24.659          |null          |51.52       |2.0283464747151054   |24 |\n",
      "|Angola   |Atlantic Ocean|2012         |20120901     |75.85             |24.363          |null          |45.12       |1.7763779107041247   |25 |\n",
      "|Angola   |Atlantic Ocean|2012         |20120801     |71.92             |22.177          |null          |35.25       |1.3877952755905512   |26 |\n",
      "|Angola   |Atlantic Ocean|2012         |20120201     |76.37             |24.652          |null          |16.45       |0.647637825312577    |27 |\n",
      "|Angola   |Atlantic Ocean|2013         |20130401     |80.57             |26.983          |null          |36.75       |1.4468503937007875   |28 |\n",
      "|Argentina|Atlantic Ocean|1996         |19960101     |82.46             |28.031          |null          |-10.31      |-0.4059055283313661  |1  |\n",
      "|Argentina|Atlantic Ocean|1997         |19970501     |69.56             |20.867          |null          |-15.01      |-0.5909448909008597  |2  |\n",
      "|Argentina|Atlantic Ocean|1997         |19970401     |72.91             |22.729          |null          |-29.31      |-1.1539369868481253  |3  |\n",
      "|Argentina|Atlantic Ocean|1997         |19970201     |80.89             |27.161          |null          |-1.91       |-0.07519684907958264 |4  |\n",
      "|Argentina|Atlantic Ocean|1998         |19980701     |67.66             |19.809          |null          |-2.81       |-0.11062991900706855 |5  |\n",
      "|Argentina|Atlantic Ocean|1998         |19980601     |64.75             |18.193          |null          |-29.21      |-1.1499999639556164  |6  |\n",
      "|Argentina|Atlantic Ocean|2000         |20001001     |77.53             |25.292          |null          |11.79       |0.4641732268446074   |7  |\n",
      "|Argentina|Atlantic Ocean|2002         |20020301     |83.17             |28.427          |null          |18.99       |0.7476377862644947   |8  |\n",
      "|Argentina|Atlantic Ocean|2002         |20020101     |82.12             |27.846          |null          |23.79       |0.9366142092727301   |9  |\n",
      "|Argentina|Atlantic Ocean|2003         |20030501     |69.44             |20.802          |null          |25.09       |0.9877952815979485   |10 |\n",
      "|Argentina|Atlantic Ocean|2003         |20030401     |74.41             |23.56           |null          |-2.61       |-0.10275590138172541 |11 |\n",
      "|Argentina|Atlantic Ocean|2003         |20030201     |81.51             |27.507          |null          |-0.55       |-0.021653543776414527|12 |\n",
      "|Argentina|Atlantic Ocean|2004         |20040701     |64.32             |17.955          |null          |9.15        |0.3602362054539478   |13 |\n",
      "|Argentina|Atlantic Ocean|2004         |20040601     |66.59             |19.217          |null          |-2.15       |-0.08464567304596188 |14 |\n",
      "|Argentina|Atlantic Ocean|2005         |20050801     |69.92             |21.067          |null          |4.89        |0.19251967978289747  |15 |\n",
      "|Argentina|Atlantic Ocean|2009         |20090501     |70.6              |21.446          |null          |15.52       |0.6110236400694359   |16 |\n",
      "|Argentina|Atlantic Ocean|2009         |20090401     |77.52             |25.288          |null          |9.62        |0.37874015297476704  |17 |\n",
      "|Argentina|Atlantic Ocean|2009         |20090201     |81.33             |27.403          |null          |19.02       |0.7488189156599871   |18 |\n",
      "|Argentina|Atlantic Ocean|2010         |20100701     |62.66             |17.034          |null          |29.12       |1.1464567259540708   |19 |\n",
      "|Argentina|Atlantic Ocean|2010         |20100601     |65.78             |18.765          |null          |23.72       |0.9338582406832477   |20 |\n",
      "|Argentina|Atlantic Ocean|2011         |20110801     |66.27             |19.041          |null          |17.92       |0.7055118140273208   |21 |\n",
      "|Argentina|Atlantic Ocean|2011         |20110301     |78.74             |25.967          |null          |26.05       |1.025590521144116    |22 |\n",
      "|Argentina|Atlantic Ocean|2012         |20121101     |80.64             |27.025          |null          |69.75       |2.7460629921259843   |23 |\n",
      "|Argentina|Atlantic Ocean|2012         |20121001     |79.46             |26.368          |null          |51.52       |2.0283464747151054   |24 |\n",
      "|Argentina|Atlantic Ocean|2012         |20120901     |74.4              |23.558          |null          |45.12       |1.7763779107041247   |25 |\n",
      "|Argentina|Atlantic Ocean|2012         |20120801     |70.37             |21.315          |null          |35.25       |1.3877952755905512   |26 |\n",
      "|Argentina|Atlantic Ocean|2012         |20120201     |83.28             |28.488          |null          |16.45       |0.647637825312577    |27 |\n",
      "|Argentina|Atlantic Ocean|2013         |20130401     |74.08             |23.376          |null          |36.75       |1.4468503937007875   |28 |\n",
      "|Australia|Indian Ocean  |1996         |19960101     |81.21             |27.339          |311886144     |1.67        |0.06574802980648252  |1  |\n",
      "|Australia|Indian Ocean  |1997         |19970501     |73.32             |22.955          |320282624     |-14.63      |-0.5759842564740519  |2  |\n",
      "|Australia|Indian Ocean  |1997         |19970401     |75.55             |24.197          |320282624     |2.97        |0.11692913498465471  |3  |\n",
      "|Australia|Indian Ocean  |1997         |19970201     |80.84             |27.131          |320282624     |-8.43       |-0.3318897757943221  |4  |\n",
      "|Australia|Indian Ocean  |1998         |19980701     |73.21             |22.897          |334075968     |12.87       |0.5066929088802788   |5  |\n",
      "|Australia|Indian Ocean  |1998         |19980601     |73.59             |23.108          |334075968     |-18.43      |-0.725590563195897   |6  |\n",
      "|Australia|Indian Ocean  |1999         |19990801     |70.02             |21.124          |343488640     |-11.23      |-0.44212596622977673 |7  |\n",
      "|Australia|Indian Ocean  |2000         |20001001     |75.47             |24.152          |349635520     |6.07        |0.23897638471107785  |8  |\n",
      "|Australia|Indian Ocean  |2000         |20000901     |72.44             |22.466          |349635520     |-31.73      |-1.2492125804030052  |9  |\n",
      "|Australia|Indian Ocean  |2002         |20020301     |80.68             |27.047          |361540800     |8.97        |0.35314961681215784  |10 |\n",
      "|Australia|Indian Ocean  |2003         |20030501     |75.54             |24.191          |369279488     |41.17       |1.6208660696435164   |11 |\n",
      "|Australia|Indian Ocean  |2003         |20030401     |78.23             |25.682          |369279488     |37.77       |1.48700789203794     |12 |\n",
      "+---------+--------------+-------------+-------------+------------------+----------------+--------------+------------+---------------------+---+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fq.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2aed592e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/26 17:10:31 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+------------------+-------------------+------------------+------------------+--------------------+------------------+-------------------+-----------------+\n",
      "|summary|country|    sea_name|     year_measured|      date_measured|avg_temp_faranheit|  avg_temp_celsius|      cumulative_co2|      sea_level_mm|   sea_level_inches|               rn|\n",
      "+-------+-------+------------+------------------+-------------------+------------------+------------------+--------------------+------------------+-------------------+-----------------+\n",
      "|  count|   2758|        2758|              2758|               2758|              2758|              2758|                 693|              2758|               2758|             2758|\n",
      "|   mean|   null|        null|2005.1145757795505|2.005176521319797E7| 72.53365482233514|22.518680928124045|2.6457746055122656E8|16.312461903814476| 0.6422229095989934|15.41841914430747|\n",
      "| stddev|   null|        null|5.7626174524599225|  57677.27896205512|15.499062830542268| 8.610538849318475| 4.469492425517778E8| 55.91360982402635|  2.201323221418358|8.712869179492145|\n",
      "|    min|Albania|Adriatic Sea|              1993|           19930701|              16.9|            -8.391|             7456007|           -347.75|-13.690944881889765|                1|\n",
      "|    max|  Yemen|Persian Gulf|              2013|           20130401|             101.1|            38.389|          2147483647|            290.95| 11.454724890040602|               33|\n",
      "+-------+-------+------------+------------------+-------------------+------------------+------------------+--------------------+------------------+-------------------+-----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [country#2966 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(country#2966 ASC NULLS FIRST, 107), ENSURE_REQUIREMENTS, [plan_id=17132]\n",
      "      +- Window [row_number() windowspecdefinition(country#2966, year_measured#2968 ASC NULLS FIRST, date_measured#2967 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#2965], [country#2966], [year_measured#2968 ASC NULLS FIRST, date_measured#2967 DESC NULLS LAST]\n",
      "         +- Sort [country#2966 ASC NULLS FIRST, year_measured#2968 ASC NULLS FIRST, date_measured#2967 DESC NULLS LAST], false, 0\n",
      "            +- Exchange hashpartitioning(country#2966, 107), ENSURE_REQUIREMENTS, [plan_id=17128]\n",
      "               +- HashAggregate(keys=[_groupingexpression#4956, sea_name#19, _groupingexpression#4957, _groupingexpression#4958], functions=[max(round(((cast((avg_temp#9 * 9.0) as double) / 5.0) + 32.0), 2)), max(avg_temp#9), max(cast(cumulative_co2#32 as int)), max(sea_level#21), max(((cast(sea_level#21 as double) * 1.0) / 25.4))])\n",
      "                  +- Exchange hashpartitioning(_groupingexpression#4956, sea_name#19, _groupingexpression#4957, _groupingexpression#4958, 107), ENSURE_REQUIREMENTS, [plan_id=17125]\n",
      "                     +- HashAggregate(keys=[_groupingexpression#4956, sea_name#19, _groupingexpression#4957, _groupingexpression#4958], functions=[partial_max(round(((cast((avg_temp#9 * 9.0) as double) / 5.0) + 32.0), 2)), partial_max(avg_temp#9), partial_max(cast(cumulative_co2#32 as int)), partial_max(sea_level#21), partial_max(((cast(sea_level#21 as double) * 1.0) / 25.4))])\n",
      "                        +- Project [avg_temp#9, sea_name#19, sea_level#21, cumulative_co2#32, coalesce(country#7, country#24, country#7) AS _groupingexpression#4956, cast(date_measured#8 as int) AS _groupingexpression#4957, year(cast(gettimestamp(date_measured#8, yyyyMMdd, TimestampType, Some(America/Los_Angeles), false) as date)) AS _groupingexpression#4958]\n",
      "                           +- BroadcastHashJoin [country#7, year(cast(gettimestamp(date_measured#8, yyyyMMdd, TimestampType, Some(America/Los_Angeles), false) as date))], [country#24, year_measured#25], LeftOuter, BuildRight, false\n",
      "                              :- Project [country#7, date_measured#8, avg_temp#9, sea_name#19, sea_level#21]\n",
      "                              :  +- BroadcastHashJoin [sea#16, date_measured#8], [sea_name#19, date_measured#20], Inner, BuildRight, false\n",
      "                              :     :- Project [country#7, date_measured#8, avg_temp#9, sea#16]\n",
      "                              :     :  +- BroadcastHashJoin [country#7], [country#15], Inner, BuildRight, false\n",
      "                              :     :     :- Project [country#7, date_measured#8, avg_temp#9]\n",
      "                              :     :     :  +- BroadcastHashJoin [country#7], [country#12], LeftOuter, BuildRight, false\n",
      "                              :     :     :     :- Filter isnotnull(date_measured#8)\n",
      "                              :     :     :     :  +- FileScan parquet spark_catalog.cmsi620_gpr3_db.global_temp_f[country#7,date_measured#8,avg_temp#9] Batched: true, DataFilters: [isnotnull(date_measured#8)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/mariamjoan/Desktop/Spark/spark-warehouse/cmsi620_gpr3_db.d..., PartitionFilters: [], PushedFilters: [IsNotNull(date_measured)], ReadSchema: struct<country:string,date_measured:string,avg_temp:float>\n",
      "                              :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=17108]\n",
      "                              :     :     :        +- Filter isnotnull(country#12)\n",
      "                              :     :     :           +- FileScan parquet spark_catalog.cmsi620_gpr3_db.global_temp_city_f[country#12] Batched: true, DataFilters: [isnotnull(country#12)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/mariamjoan/Desktop/Spark/spark-warehouse/cmsi620_gpr3_db.d..., PartitionFilters: [], PushedFilters: [IsNotNull(country)], ReadSchema: struct<country:string>\n",
      "                              :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=17112]\n",
      "                              :     :        +- Filter (isnotnull(country#15) AND isnotnull(sea#16))\n",
      "                              :     :           +- FileScan parquet spark_catalog.cmsi620_gpr3_db.sea_level_country_f[country#15,sea#16] Batched: true, DataFilters: [isnotnull(country#15), isnotnull(sea#16)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/mariamjoan/Desktop/Spark/spark-warehouse/cmsi620_gpr3_db.d..., PartitionFilters: [], PushedFilters: [IsNotNull(country), IsNotNull(sea)], ReadSchema: struct<country:string,sea:string>\n",
      "                              :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false], input[1, string, false]),false), [plan_id=17116]\n",
      "                              :        +- Filter (isnotnull(sea_name#19) AND isnotnull(date_measured#20))\n",
      "                              :           +- FileScan parquet spark_catalog.cmsi620_gpr3_db.sea_level_f[sea_name#19,date_measured#20,sea_level#21] Batched: true, DataFilters: [isnotnull(sea_name#19), isnotnull(date_measured#20)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/mariamjoan/Desktop/Spark/spark-warehouse/cmsi620_gpr3_db.d..., PartitionFilters: [], PushedFilters: [IsNotNull(sea_name), IsNotNull(date_measured)], ReadSchema: struct<sea_name:string,date_measured:string,sea_level:float>\n",
      "                              +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false], input[1, int, false]),false), [plan_id=17120]\n",
      "                                 +- Filter (isnotnull(country#24) AND isnotnull(year_measured#25))\n",
      "                                    +- FileScan parquet spark_catalog.cmsi620_gpr3_db.global_co2_emissions_country_sum_f[country#24,year_measured#25,cumulative_co2#32] Batched: true, DataFilters: [isnotnull(country#24), isnotnull(year_measured#25)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/mariamjoan/Desktop/Spark/spark-warehouse/cmsi620_gpr3_db.d..., PartitionFilters: [], PushedFilters: [IsNotNull(country), IsNotNull(year_measured)], ReadSchema: struct<country:string,year_measured:int,cumulative_co2:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fq.describe().show()\n",
    "fq.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7791fa7",
   "metadata": {},
   "source": [
    "# CRUD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660cf54",
   "metadata": {},
   "source": [
    "### 1. Create: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5965695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE global_temp_city_d (\n",
    "    dt STRING, \n",
    "    AverageTemperature FLOAT, \n",
    "    AverageTemperatureUncertainty FLOAT, \n",
    "    City STRING, \n",
    "    Country STRING, \n",
    "    Latitude STRING, \n",
    "    Longitude STRING\n",
    ")\n",
    "USING CSV\n",
    "PARTITIONED BY (snapshot_date, country)\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "WITH SERDEPROPERTIES (\n",
    "  'separatorChar' = ',',\n",
    "  'quoteChar' = '\"',\n",
    "  'escapeChar' = '\\\\'\n",
    ")\n",
    "STORED AS TEXTFILE\n",
    "LOCATION file:///Users/mariamjoan/Desktop/Spark/spark-warehouse';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ababa1a",
   "metadata": {},
   "source": [
    "### 2. Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d76c8d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|max_avg_temp|\n",
      "+------------+\n",
      "|      39.651|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select max(avg_temp) max_avg_temp from {DATABASE}.global_temp_f \").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c32fe",
   "metadata": {},
   "source": [
    "### 3. **Update** \n",
    "\n",
    "Spark does not have a concept of UPDATE in the traditional RDBMS sense. You can only perform \"update\" like operations on the RDD or Spark DataFrame but, you will need to perform a WRITE action in order for that change to be reflected back to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3f01258",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_df = spark.sql(f\"select * from {DATABASE}.global_temp_f \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6543f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_df = update_df.withColumn(\"year\", year(\"date_collected\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103667b",
   "metadata": {},
   "source": [
    "### 4. Delete \n",
    "Spark does not have a concept of DELETE because Spark DataFrames are an immutable object, and we cannot delete rows “in place”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f68892cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = spark.sql(f\"\"\" select * from {DATABASE}.global_temp_f where 1=1 and date_measured > 20000101\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7ece20bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+--------+--------------------+\n",
      "|       city|      country|date_measured|avg_temp|avg_temp_uncertainty|\n",
      "+-----------+-------------+-------------+--------+--------------------+\n",
      "|Baton Rouge|United States|     20000201|  16.108|               0.265|\n",
      "|Baton Rouge|United States|     20000301|  19.299|               0.327|\n",
      "|Baton Rouge|United States|     20000401|   20.13|               0.194|\n",
      "|Baton Rouge|United States|     20000501|  26.323|               0.271|\n",
      "+-----------+-------------+-------------+--------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_df= ddf.filter('country = \"United States\" ')\n",
    "us_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fee3bae",
   "metadata": {},
   "source": [
    "# ACID Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea28d6c",
   "metadata": {},
   "source": [
    "### 1. Atomicity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f29a5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd_path = !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "deb8a8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------------+--------+--------------------+---------------------+----+------------------+\n",
      "| city|country|date_collected|avg_temp|avg_temp_uncertainty|date_collected_format|year|avg_temp_farenheit|\n",
      "+-----+-------+--------------+--------+--------------------+---------------------+----+------------------+\n",
      "|Namur|Belgium|    1866-07-01|  16.911|               0.652|             18660701|1866|62.439797973632814|\n",
      "|Namur|Belgium|    1866-08-01|  15.738|               0.742|             18660801|1866|60.328399658203125|\n",
      "+-----+-------+--------------+--------+--------------------+---------------------+----+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "update_df.withColumn(\"avg_temp_farenheit\", expr(\"(avg_temp * 9/5) + 32\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa2ae671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed DataFrame:\n",
      "+-----+-------+--------------+--------+--------------------+---------------------+----+------------------+\n",
      "| city|country|date_collected|avg_temp|avg_temp_uncertainty|date_collected_format|year|avg_temp_farenheit|\n",
      "+-----+-------+--------------+--------+--------------------+---------------------+----+------------------+\n",
      "|Namur|Belgium|    1866-07-01|  16.911|               0.652|             18660701|1866|62.439797973632814|\n",
      "|Namur|Belgium|    1866-08-01|  15.738|               0.742|             18660801|1866|60.328399658203125|\n",
      "|Namur|Belgium|    1866-09-01|  14.752|               0.793|             18660901|1866| 58.55360107421875|\n",
      "+-----+-------+--------------+--------+--------------------+---------------------+----+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Exception: An error occurred while calling o170.parquet. Trace:\n",
      "py4j.Py4JException: Method parquet([class java.util.ArrayList]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:835)\n",
      "\n",
      "\n",
      "Rolling back changes...\n",
      "Recovered DataFrame:\n",
      "+-----+-------+--------------+--------+--------------------+---------------------+----+------------------+\n",
      "| city|country|date_collected|avg_temp|avg_temp_uncertainty|date_collected_format|year|avg_temp_farenheit|\n",
      "+-----+-------+--------------+--------+--------------------+---------------------+----+------------------+\n",
      "|Namur|Belgium|    1866-07-01|  16.911|               0.652|             18660701|1866|62.439797973632814|\n",
      "|Namur|Belgium|    1866-08-01|  15.738|               0.742|             18660801|1866|60.328399658203125|\n",
      "|Namur|Belgium|    1866-09-01|  14.752|               0.793|             18660901|1866| 58.55360107421875|\n",
      "+-----+-------+--------------+--------+--------------------+---------------------+----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Atomic Operation 1: Transform the DataFrame\n",
    "    update_fdf = update_df.withColumn(\"avg_temp_farenheit\", expr(\"(avg_temp * 9/5) + 32\"))\n",
    "\n",
    "    print(\"Transformed DataFrame:\")\n",
    "    update_fdf.show(3)\n",
    "\n",
    "    # Atomic Operation 2: Write the transformed DataFrame to a temporary file\n",
    "    update_fdf.write.mode(\"overwrite\").parquet(pwd_path)\n",
    "\n",
    "    # Atomic Operation 3: Simulate a failure (for demonstration purposes)\n",
    "    raise Exception(\"Simulated failure\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Log the exception (simulating recovery or rollback)\n",
    "    print(f\"Exception: {e}\")\n",
    "    print(\"Rolling back changes...\")\n",
    "\n",
    "finally:\n",
    "    # Atomic Operation 4: Read the original DataFrame from the temporary file (recovery)\n",
    "    recovered_update_fdf = update_fdf\n",
    "\n",
    "    # Show the recovered DataFrame\n",
    "    print(\"Recovered DataFrame:\")\n",
    "    recovered_update_fdf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6708115f",
   "metadata": {},
   "source": [
    "### 2. Consistency, 3. Isolation or Idempotency, 4. Durability "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7266b64",
   "metadata": {},
   "source": [
    "Spark is mainly designed for distributed processing of data, and transactional guarantees are handled at data storage level like as with HADOOP or distributed file systems, or databases that can be used in conjunction with Spark not by Spark alone however, Spark does by nature resolve many of these operations such as consistency, atomicity and durability. Spark can't require commits or writes to be isolated instances or isolated from other transactions without negative impact. Spark has multiple layers of commits, from task and job level, and these can happen at different times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929535f",
   "metadata": {},
   "source": [
    "# Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f011e4f4",
   "metadata": {},
   "source": [
    "This configuration `spark.sql.files.maxRecordsPerFile` controls the maximum number of records to write out to a single file when saving a DataFrame to a distributed file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "93f7743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxRecordsPerFile\", \"1000\")\n",
    "spark.conf.set(\"spark.default.parallelism\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9fb7b5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[city: string, country: string, date_measured: string, avg_temp: float, avg_temp_uncertainty: float]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_repartition_replicated = ddf.repartition(8)  # update # of partitions to cache for faster memory access  \n",
    "ddf_repartition_replicated.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3f816161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------+--------+--------------------+\n",
      "|       city|             country|date_measured|avg_temp|avg_temp_uncertainty|\n",
      "+-----------+--------------------+-------------+--------+--------------------+\n",
      "|      Cagua|           Venezuela|     20000401|  26.758|               0.369|\n",
      "|   Borujerd|                Iran|     20050601|  21.506|               0.534|\n",
      "|Bournemouth|      United Kingdom|     20120501|  12.128|               0.253|\n",
      "|    Bristol|      United Kingdom|     20010601|    14.4|               0.236|\n",
      "|    Bologna|               Italy|     20060201|   4.156|               0.299|\n",
      "|    Brikama|              Gambia|     20100801|  28.567|               0.244|\n",
      "|    Bikaner|               India|     20040101|  15.014|               0.436|\n",
      "|     Brugge|             Belgium|     20061001|  14.137|               0.148|\n",
      "|     Bozhou|               China|     20090501|  21.443|               0.485|\n",
      "|     Bukavu|Congo (Democratic...|     20130801|  21.954|               0.688|\n",
      "|    Bhiwani|               India|     20040801|  30.071|               0.456|\n",
      "|      Breda|         Netherlands|     20080601|   16.37|               0.245|\n",
      "| Cabanatuan|         Philippines|     20000901|  26.642|               0.255|\n",
      "|     Beirut|             Lebanon|     20130601|  24.131|               0.856|\n",
      "|     Brugge|             Belgium|     20050401|  10.668|               0.227|\n",
      "|   Biên Hòa|             Vietnam|     20110401|  27.888|               0.327|\n",
      "|     Bokaro|               India|     20110501|  31.008|                0.29|\n",
      "|     Bratsk|              Russia|     20090901|   6.952|               0.279|\n",
      "|    Birigui|              Brazil|     20120201|  27.141|               0.272|\n",
      "|   Bulawayo|            Zimbabwe|     20021101|  22.419|               0.271|\n",
      "+-----------+--------------------+-------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddf_repartition_replicated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7653e1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32629e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
